import requests
from bs4 import BeautifulSoup

def download_pdfs(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    #find download buttons, exctract download link from "onclick" event/attribute
    for index, button in enumerate(soup.find_all("button", {"onclick": True})):
        #style of the onclick url depends on websites
        pdf_url = button["onclick"].split("'")[1]
        if not pdf_url.startswith("http"):
            pdf_url="https://"+soup.find_all('button')[0]['onclick'].split('//')[1].split('?')[0]
        response = requests.get(pdf_url)
        #save the responce which is pdf file
        #open(r"path_to_save"+"\\"+str(index)+".pdf", "wb").write(response.content)

    #delete .js or etc extemsions from url end
    if url.split("/")[-1].find("."):
        lastPart=url.split("/")[-1].split(".")[0]
        url=url.split(lastPart)[0]+lastPart

    #find all link in response
    for index, link in enumerate(soup.find_all("a")):
        try:
            #find links with ".pdf" at the end.
            if link.get("href").endswith(".pdf"):
                pdf_url = link.get("href")
                #if link starts with "http", we download it; otherwise we will concatenate url + anchor/href
                if not pdf_url.startswith("http"):
                    #try new url, if fails, try to delete common parts
                    if requests.get(url+"/"+pdf_url).status_code==404:
                        lastPart=url.split("/")[-1].split(".")[0]
                        url=url.split(lastPart)[0][:-1]
                    pdf_url=url+"/"+pdf_url
                print(pdf_url)
                response = requests.get(pdf_url)
                print(response.status_code)
                #save the responce which is pdf file
                #open(r"path_to_save"+"\\"+str(index)+".pdf", "wb").write(response.content)
        except:
            pass
download_pdfs("http://example.com")
